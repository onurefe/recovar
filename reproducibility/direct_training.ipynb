{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1cfd6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 18:02:58.549308: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-04 18:02:58.568191: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-04 18:02:58.568213: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-04 18:02:58.568226: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-04 18:02:58.571919: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from direct_trainer import DirectTrainer\n",
    "from recovar import RepresentationLearningSingleAutoencoder, RepresentationLearningDenoisingSingleAutoencoder, RepresentationLearningMultipleAutoencoder\n",
    "from config import (STEAD_TIME_WINDOW, INSTANCE_TIME_WINDOW, WINDOW_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feaf54af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 18:03:00.869252: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 18:03:00.897386: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 18:03:00.897477: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 18:03:00.898319: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 18:03:00.898373: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 18:03:00.898415: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 18:03:01.171563: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 18:03:01.171647: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 18:03:01.171697: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 18:03:01.171734: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22141 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "DATASET = 'stead'\n",
    "MODEL = RepresentationLearningMultipleAutoencoder()\n",
    "EPOCH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1cc389",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = \"/home/ege/recovar_data_preprocessed/stead_pseudorandom_state_ne_subsampled/SUBSAMPLED_100_NOISE_40.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa67e348",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = DirectTrainer(dataset=DATASET,\n",
    "    dataset_time_window=STEAD_TIME_WINDOW,\n",
    "    model_time_window=WINDOW_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02b19bb",
   "metadata": {},
   "source": [
    "### Option 1. Preprocess the whole data, load it into memory and separate train/val files from there on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c3cea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.create_subsampled_datasets(\n",
    " #   dataset='stead',\n",
    "  #  output_dir=PREPROCESSED_DATASET_DIRECTORY,\n",
    "   # noise_percentages=[0, 10, 20, 90, 100],\n",
    "    #subsampling_factor=1.0, \n",
    "    #maintain_constant_size=True\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6092e1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 176448 samples (EQ: 105879, NO: 70569)\n",
      "Val: 58816 samples (EQ: 35293, NO: 23523)\n",
      "\n",
      "Starting training...\n",
      "Train samples: 176448\n",
      "Val samples: 58816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 17:09:25.498332: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 17:09:45.324651: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1f0f3790 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-07-04 17:09:45.324669: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3090 Ti, Compute Capability 8.6\n",
      "2025-07-04 17:09:45.332786: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-07-04 17:09:45.404756: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "690/690 [==============================] - ETA: 0s - loss: 1.6378\n",
      "Epoch 1: val_loss improved from inf to 1.45220, saving model to checkpoints/SUBSAMPLED_100_NOISE_40_best_model.h5\n",
      "690/690 [==============================] - 119s 139ms/step - loss: 1.6378 - val_loss: 1.4522 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "690/690 [==============================] - ETA: 0s - loss: 1.3034\n",
      "Epoch 2: val_loss improved from 1.45220 to 1.25263, saving model to checkpoints/SUBSAMPLED_100_NOISE_40_best_model.h5\n",
      "690/690 [==============================] - 93s 135ms/step - loss: 1.3034 - val_loss: 1.2526 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "690/690 [==============================] - ETA: 0s - loss: 1.1491\n",
      "Epoch 3: val_loss improved from 1.25263 to 1.22802, saving model to checkpoints/SUBSAMPLED_100_NOISE_40_best_model.h5\n",
      "690/690 [==============================] - 93s 135ms/step - loss: 1.1491 - val_loss: 1.2280 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "690/690 [==============================] - ETA: 0s - loss: 1.0843\n",
      "Epoch 4: val_loss improved from 1.22802 to 1.08144, saving model to checkpoints/SUBSAMPLED_100_NOISE_40_best_model.h5\n",
      "690/690 [==============================] - 93s 135ms/step - loss: 1.0843 - val_loss: 1.0814 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "690/690 [==============================] - ETA: 0s - loss: 1.0515\n",
      "Epoch 5: val_loss improved from 1.08144 to 1.05722, saving model to checkpoints/SUBSAMPLED_100_NOISE_40_best_model.h5\n",
      "690/690 [==============================] - 93s 135ms/step - loss: 1.0515 - val_loss: 1.0572 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "690/690 [==============================] - ETA: 0s - loss: 1.0282\n",
      "Epoch 6: val_loss improved from 1.05722 to 1.02187, saving model to checkpoints/SUBSAMPLED_100_NOISE_40_best_model.h5\n",
      "690/690 [==============================] - 93s 135ms/step - loss: 1.0282 - val_loss: 1.0219 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "690/690 [==============================] - ETA: 0s - loss: 1.0027\n",
      "Epoch 7: val_loss improved from 1.02187 to 1.00491, saving model to checkpoints/SUBSAMPLED_100_NOISE_40_best_model.h5\n",
      "690/690 [==============================] - 93s 135ms/step - loss: 1.0027 - val_loss: 1.0049 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "690/690 [==============================] - ETA: 0s - loss: 0.9814\n",
      "Epoch 8: val_loss improved from 1.00491 to 0.97638, saving model to checkpoints/SUBSAMPLED_100_NOISE_40_best_model.h5\n",
      "690/690 [==============================] - 93s 135ms/step - loss: 0.9814 - val_loss: 0.9764 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "690/690 [==============================] - ETA: 0s - loss: 0.9656\n",
      "Epoch 9: val_loss did not improve from 0.97638\n",
      "690/690 [==============================] - 93s 135ms/step - loss: 0.9656 - val_loss: 1.0015 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "690/690 [==============================] - ETA: 0s - loss: 0.9756\n",
      "Epoch 10: val_loss did not improve from 0.97638\n",
      "690/690 [==============================] - 93s 135ms/step - loss: 0.9756 - val_loss: 0.9875 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "history = trainer.train(\n",
    "    model=MODEL,\n",
    "    dataset_path=TRAIN_DATA_PATH,\n",
    "    epochs=EPOCH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dc4022",
   "metadata": {},
   "source": [
    "### Option 2. Create datasets with train/val/test splits saved as separate files and train on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39c4dd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = DirectTrainer(\n",
    "    dataset=DATASET,\n",
    "    dataset_time_window=STEAD_TIME_WINDOW,\n",
    "    model_time_window=WINDOW_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acb5338f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ege/recovar/reproducibility/direct_trainer.py:541: DtypeWarning: Columns (21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  metadata = pd.read_csv(metadata_csv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples before subsampling: EQ=1030231, NO=235426\n",
      "After subsampling (1%): EQ=10302, NO=2354\n",
      "No noise percentages specified. Creating full dataset with all available samples...\n",
      "\n",
      "Creating train/val/test splits for: FULL_DATASET_SUBSAMPLED_1\n",
      "Total samples: 12656 (EQ: 10302, NO: 2354)\n",
      "Processed 1/49 batches...\n",
      "Processed 11/49 batches...\n",
      "Processed 21/49 batches...\n",
      "Processed 31/49 batches...\n",
      "Processed 41/49 batches...\n",
      "Saved train: preprocessed_data/stead_splits/FULL_DATASET_SUBSAMPLED_1_train.hdf5 - 6271 samples (EQ: 5150, NO: 1121)\n",
      "Saved val: preprocessed_data/stead_splits/FULL_DATASET_SUBSAMPLED_1_val.hdf5 - 2509 samples (EQ: 2061, NO: 448)\n",
      "Saved test: preprocessed_data/stead_splits/FULL_DATASET_SUBSAMPLED_1_test.hdf5 - 3764 samples (EQ: 3091, NO: 673)\n",
      "Created full dataset: EQ=10302, NO=2354, Total=12656\n"
     ]
    }
   ],
   "source": [
    "trainer.create_subsampled_datasets(\n",
    "    dataset='stead',\n",
    "    output_dir='preprocessed_data/stead_splits',\n",
    "    noise_percentages=[], \n",
    "    subsampling_factor=0.01, \n",
    "    maintain_constant_size=False,\n",
    "    save_train_val_test_splits=True, \n",
    "    val_ratio=0.2, \n",
    "    test_ratio=0.3,\n",
    "    random_state_mode='pseudorandom',\n",
    "    base_random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c88046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "Train batches: 2\n",
      "Val batches: 1\n",
      "Using HDF5Generator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 17:53:06.122107: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 17:53:24.424524: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fbae400de00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-07-04 17:53:24.424539: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3090 Ti, Compute Capability 8.6\n",
      "2025-07-04 17:53:24.427643: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-07-04 17:53:24.474209: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 2.7898 \n",
      "Epoch 1: val_loss improved from inf to 2.42737, saving model to checkpoints/FULL_DATASET_SUBSAMPLED_0_train_best_model.h5\n",
      "2/2 [==============================] - 27s 5s/step - loss: 2.7898 - val_loss: 2.4274 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.6873\n",
      "Epoch 2: val_loss improved from 2.42737 to 2.38013, saving model to checkpoints/FULL_DATASET_SUBSAMPLED_0_train_best_model.h5\n",
      "2/2 [==============================] - 0s 300ms/step - loss: 2.6873 - val_loss: 2.3801 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.5792\n",
      "Epoch 3: val_loss improved from 2.38013 to 2.33944, saving model to checkpoints/FULL_DATASET_SUBSAMPLED_0_train_best_model.h5\n",
      "2/2 [==============================] - 0s 296ms/step - loss: 2.5792 - val_loss: 2.3394 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.4917\n",
      "Epoch 4: val_loss improved from 2.33944 to 2.32055, saving model to checkpoints/FULL_DATASET_SUBSAMPLED_0_train_best_model.h5\n",
      "2/2 [==============================] - 0s 301ms/step - loss: 2.4917 - val_loss: 2.3206 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.4460\n",
      "Epoch 5: val_loss improved from 2.32055 to 2.30861, saving model to checkpoints/FULL_DATASET_SUBSAMPLED_0_train_best_model.h5\n",
      "2/2 [==============================] - 1s 650ms/step - loss: 2.4460 - val_loss: 2.3086 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.3942\n",
      "Epoch 6: val_loss improved from 2.30861 to 2.29202, saving model to checkpoints/FULL_DATASET_SUBSAMPLED_0_train_best_model.h5\n",
      "2/2 [==============================] - 1s 382ms/step - loss: 2.3942 - val_loss: 2.2920 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.3677\n",
      "Epoch 7: val_loss improved from 2.29202 to 2.26888, saving model to checkpoints/FULL_DATASET_SUBSAMPLED_0_train_best_model.h5\n",
      "2/2 [==============================] - 0s 298ms/step - loss: 2.3677 - val_loss: 2.2689 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.3685\n",
      "Epoch 8: val_loss improved from 2.26888 to 2.25247, saving model to checkpoints/FULL_DATASET_SUBSAMPLED_0_train_best_model.h5\n",
      "2/2 [==============================] - 0s 301ms/step - loss: 2.3685 - val_loss: 2.2525 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.3015\n",
      "Epoch 9: val_loss improved from 2.25247 to 2.24061, saving model to checkpoints/FULL_DATASET_SUBSAMPLED_0_train_best_model.h5\n",
      "2/2 [==============================] - 0s 354ms/step - loss: 2.3015 - val_loss: 2.2406 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.2790\n",
      "Epoch 10: val_loss improved from 2.24061 to 2.23684, saving model to checkpoints/FULL_DATASET_SUBSAMPLED_0_train_best_model.h5\n",
      "2/2 [==============================] - 0s 303ms/step - loss: 2.2790 - val_loss: 2.2368 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.2434\n",
      "Epoch 11: val_loss did not improve from 2.23684\n",
      "2/2 [==============================] - 0s 163ms/step - loss: 2.2434 - val_loss: 2.2443 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.2146\n",
      "Epoch 12: val_loss improved from 2.23684 to 2.23151, saving model to checkpoints/FULL_DATASET_SUBSAMPLED_0_train_best_model.h5\n",
      "2/2 [==============================] - 0s 299ms/step - loss: 2.2146 - val_loss: 2.2315 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.2036\n",
      "Epoch 13: val_loss did not improve from 2.23151\n",
      "2/2 [==============================] - 0s 164ms/step - loss: 2.2036 - val_loss: 2.2406 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.1535\n",
      "Epoch 14: val_loss did not improve from 2.23151\n",
      "2/2 [==============================] - 0s 249ms/step - loss: 2.1535 - val_loss: 2.2581 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.1212\n",
      "Epoch 15: val_loss did not improve from 2.23151\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "2/2 [==============================] - 0s 168ms/step - loss: 2.1212 - val_loss: 2.2859 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0842\n",
      "Epoch 16: val_loss did not improve from 2.23151\n",
      "2/2 [==============================] - 0s 164ms/step - loss: 2.0842 - val_loss: 2.3005 - lr: 5.0000e-04\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0664\n",
      "Epoch 17: val_loss did not improve from 2.23151\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "2/2 [==============================] - 0s 217ms/step - loss: 2.0664 - val_loss: 2.3097 - lr: 5.0000e-04\n",
      "Epoch 17: early stopping\n"
     ]
    }
   ],
   "source": [
    "model = RepresentationLearningMultipleAutoencoder()\n",
    "\n",
    "history = trainer.train(\n",
    "    model=model,\n",
    "    train_dataset_path='preprocessed_data/stead_splits/FULL_DATASET_SUBSAMPLED_1_train.hdf5',\n",
    "    val_dataset_path='preprocessed_data/stead_splits/FULL_DATASET_SUBSAMPLED_1_val.hdf5',\n",
    "    test_dataset_path='preprocessed_data/stead_splits/FULL_DATASET_SUBSAMPLED_1_test.hdf5',  # Optional\n",
    "    epochs=10,\n",
    "    batch_size=256,\n",
    "    learning_rate=1e-3,\n",
    "    use_hdf5_generator=True  \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RECOVAR_EGE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
