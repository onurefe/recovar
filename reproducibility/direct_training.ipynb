{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1cfd6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-05 10:51:06.033999: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-05 10:51:06.052500: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-05 10:51:06.052518: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-05 10:51:06.052530: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-05 10:51:06.056055: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from direct_trainer import DirectTrainer\n",
    "from recovar import RepresentationLearningSingleAutoencoder, RepresentationLearningDenoisingSingleAutoencoder, RepresentationLearningMultipleAutoencoder\n",
    "from config import (STEAD_TIME_WINDOW, INSTANCE_TIME_WINDOW, WINDOW_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feaf54af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-05 10:51:07.831802: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-05 10:51:07.846821: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-05 10:51:07.846904: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-05 10:51:07.848182: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-05 10:51:07.848249: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-05 10:51:07.848290: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-05 10:51:08.125228: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-05 10:51:08.125313: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-05 10:51:08.125363: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-05 10:51:08.125400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22141 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "DATASET = 'stead'\n",
    "MODEL = RepresentationLearningMultipleAutoencoder()\n",
    "EPOCH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1cc389",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = \"/home/ege/recovar_data_preprocessed/stead_pseudorandom_state_ne_subsampled/SUBSAMPLED_100_NOISE_40.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa67e348",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = DirectTrainer(dataset=DATASET,\n",
    "    dataset_time_window=STEAD_TIME_WINDOW,\n",
    "    model_time_window=WINDOW_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02b19bb",
   "metadata": {},
   "source": [
    "### Option 1. Preprocess the whole data, load it into memory and separate train/val files from there on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c3cea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.create_subsampled_datasets(\n",
    " #   dataset='stead',\n",
    "  #  output_dir=PREPROCESSED_DATASET_DIRECTORY,\n",
    "   # noise_percentages=[0, 10, 20, 90, 100],\n",
    "    #subsampling_factor=1.0, \n",
    "    #maintain_constant_size=True\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6092e1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 176448 samples (EQ: 105879, NO: 70569)\n",
      "Val: 58816 samples (EQ: 35293, NO: 23523)\n",
      "\n",
      "Starting training...\n",
      "Train samples: 176448\n",
      "Val samples: 58816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 17:09:25.498332: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 17:09:45.324651: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1f0f3790 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-07-04 17:09:45.324669: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3090 Ti, Compute Capability 8.6\n",
      "2025-07-04 17:09:45.332786: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-07-04 17:09:45.404756: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "690/690 [==============================] - ETA: 0s - loss: 1.6378\n",
      "Epoch 1: val_loss improved from inf to 1.45220, saving model to checkpoints/SUBSAMPLED_100_NOISE_40_best_model.h5\n",
      "690/690 [==============================] - 119s 139ms/step - loss: 1.6378 - val_loss: 1.4522 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "690/690 [==============================] - ETA: 0s - loss: 1.3034\n",
      "Epoch 2: val_loss improved from 1.45220 to 1.25263, saving model to checkpoints/SUBSAMPLED_100_NOISE_40_best_model.h5\n",
      "690/690 [==============================] - 93s 135ms/step - loss: 1.3034 - val_loss: 1.2526 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "690/690 [==============================] - ETA: 0s - loss: 1.1491\n",
      "Epoch 3: val_loss improved from 1.25263 to 1.22802, saving model to checkpoints/SUBSAMPLED_100_NOISE_40_best_model.h5\n",
      "690/690 [==============================] - 93s 135ms/step - loss: 1.1491 - val_loss: 1.2280 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "690/690 [==============================] - ETA: 0s - loss: 1.0843\n",
      "Epoch 4: val_loss improved from 1.22802 to 1.08144, saving model to checkpoints/SUBSAMPLED_100_NOISE_40_best_model.h5\n",
      "690/690 [==============================] - 93s 135ms/step - loss: 1.0843 - val_loss: 1.0814 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "690/690 [==============================] - ETA: 0s - loss: 1.0515\n",
      "Epoch 5: val_loss improved from 1.08144 to 1.05722, saving model to checkpoints/SUBSAMPLED_100_NOISE_40_best_model.h5\n",
      "690/690 [==============================] - 93s 135ms/step - loss: 1.0515 - val_loss: 1.0572 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "690/690 [==============================] - ETA: 0s - loss: 1.0282\n",
      "Epoch 6: val_loss improved from 1.05722 to 1.02187, saving model to checkpoints/SUBSAMPLED_100_NOISE_40_best_model.h5\n",
      "690/690 [==============================] - 93s 135ms/step - loss: 1.0282 - val_loss: 1.0219 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "690/690 [==============================] - ETA: 0s - loss: 1.0027\n",
      "Epoch 7: val_loss improved from 1.02187 to 1.00491, saving model to checkpoints/SUBSAMPLED_100_NOISE_40_best_model.h5\n",
      "690/690 [==============================] - 93s 135ms/step - loss: 1.0027 - val_loss: 1.0049 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "690/690 [==============================] - ETA: 0s - loss: 0.9814\n",
      "Epoch 8: val_loss improved from 1.00491 to 0.97638, saving model to checkpoints/SUBSAMPLED_100_NOISE_40_best_model.h5\n",
      "690/690 [==============================] - 93s 135ms/step - loss: 0.9814 - val_loss: 0.9764 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "690/690 [==============================] - ETA: 0s - loss: 0.9656\n",
      "Epoch 9: val_loss did not improve from 0.97638\n",
      "690/690 [==============================] - 93s 135ms/step - loss: 0.9656 - val_loss: 1.0015 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "690/690 [==============================] - ETA: 0s - loss: 0.9756\n",
      "Epoch 10: val_loss did not improve from 0.97638\n",
      "690/690 [==============================] - 93s 135ms/step - loss: 0.9756 - val_loss: 0.9875 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "history = trainer.train(\n",
    "    model=MODEL,\n",
    "    dataset_path=TRAIN_DATA_PATH,\n",
    "    epochs=EPOCH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dc4022",
   "metadata": {},
   "source": [
    "### Option 2. Create datasets with train/val/test splits saved as separate files and train on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39c4dd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = DirectTrainer(\n",
    "    dataset=DATASET,\n",
    "    dataset_time_window=STEAD_TIME_WINDOW,\n",
    "    model_time_window=WINDOW_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb5338f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ege/recovar/reproducibility/direct_trainer.py:534: DtypeWarning: Columns (21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  metadata = pd.read_csv(metadata_csv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples before subsampling: EQ=1030231, NO=235426\n",
      "No noise percentages specified. Creating full dataset with all available samples...\n",
      "\n",
      "Creating train/val/test splits for: FULL_DATASET_SUBSAMPLED_100\n",
      "Total samples: 1265657 (EQ: 1030231, NO: 235426)\n",
      "Processed 1/4943 batches...\n",
      "Processed 11/4943 batches...\n",
      "Processed 21/4943 batches...\n",
      "Processed 31/4943 batches...\n",
      "Processed 41/4943 batches...\n",
      "Processed 51/4943 batches...\n",
      "Processed 61/4943 batches...\n",
      "Processed 71/4943 batches...\n",
      "Processed 81/4943 batches...\n",
      "Processed 91/4943 batches...\n",
      "Processed 101/4943 batches...\n",
      "Processed 111/4943 batches...\n",
      "Processed 121/4943 batches...\n",
      "Processed 131/4943 batches...\n",
      "Processed 141/4943 batches...\n",
      "Processed 151/4943 batches...\n",
      "Processed 161/4943 batches...\n",
      "Processed 171/4943 batches...\n",
      "Processed 181/4943 batches...\n",
      "Processed 191/4943 batches...\n",
      "Processed 201/4943 batches...\n",
      "Processed 211/4943 batches...\n",
      "Processed 221/4943 batches...\n",
      "Processed 231/4943 batches...\n",
      "Processed 241/4943 batches...\n",
      "Processed 251/4943 batches...\n",
      "Processed 261/4943 batches...\n",
      "Processed 271/4943 batches...\n",
      "Processed 281/4943 batches...\n",
      "Processed 291/4943 batches...\n",
      "Processed 301/4943 batches...\n",
      "Processed 311/4943 batches...\n",
      "Processed 321/4943 batches...\n",
      "Processed 331/4943 batches...\n",
      "Processed 341/4943 batches...\n",
      "Processed 351/4943 batches...\n",
      "Processed 361/4943 batches...\n",
      "Processed 371/4943 batches...\n",
      "Processed 381/4943 batches...\n",
      "Processed 391/4943 batches...\n",
      "Processed 401/4943 batches...\n",
      "Processed 411/4943 batches...\n",
      "Processed 421/4943 batches...\n",
      "Processed 431/4943 batches...\n",
      "Processed 441/4943 batches...\n",
      "Processed 451/4943 batches...\n",
      "Processed 461/4943 batches...\n",
      "Processed 471/4943 batches...\n",
      "Processed 481/4943 batches...\n",
      "Processed 491/4943 batches...\n",
      "Processed 501/4943 batches...\n",
      "Processed 511/4943 batches...\n",
      "Processed 521/4943 batches...\n",
      "Processed 531/4943 batches...\n",
      "Processed 541/4943 batches...\n",
      "Processed 551/4943 batches...\n",
      "Processed 561/4943 batches...\n",
      "Processed 571/4943 batches...\n",
      "Processed 581/4943 batches...\n",
      "Processed 591/4943 batches...\n",
      "Processed 601/4943 batches...\n",
      "Processed 611/4943 batches...\n",
      "Processed 621/4943 batches...\n",
      "Processed 631/4943 batches...\n",
      "Processed 641/4943 batches...\n",
      "Processed 651/4943 batches...\n",
      "Processed 661/4943 batches...\n",
      "Processed 671/4943 batches...\n",
      "Processed 681/4943 batches...\n",
      "Processed 691/4943 batches...\n",
      "Processed 701/4943 batches...\n",
      "Processed 711/4943 batches...\n",
      "Processed 721/4943 batches...\n",
      "Processed 731/4943 batches...\n",
      "Processed 741/4943 batches...\n",
      "Processed 751/4943 batches...\n",
      "Processed 761/4943 batches...\n",
      "Processed 771/4943 batches...\n",
      "Processed 781/4943 batches...\n",
      "Processed 791/4943 batches...\n",
      "Processed 801/4943 batches...\n",
      "Processed 811/4943 batches...\n",
      "Processed 821/4943 batches...\n",
      "Processed 831/4943 batches...\n",
      "Processed 841/4943 batches...\n",
      "Processed 851/4943 batches...\n",
      "Processed 861/4943 batches...\n",
      "Processed 871/4943 batches...\n",
      "Processed 881/4943 batches...\n",
      "Processed 891/4943 batches...\n",
      "Processed 901/4943 batches...\n",
      "Processed 911/4943 batches...\n",
      "Processed 921/4943 batches...\n",
      "Processed 931/4943 batches...\n",
      "Processed 941/4943 batches...\n",
      "Processed 951/4943 batches...\n",
      "Processed 961/4943 batches...\n",
      "Processed 971/4943 batches...\n",
      "Processed 981/4943 batches...\n",
      "Processed 991/4943 batches...\n",
      "Processed 1001/4943 batches...\n",
      "Processed 1011/4943 batches...\n",
      "Processed 1021/4943 batches...\n",
      "Processed 1031/4943 batches...\n",
      "Processed 1041/4943 batches...\n",
      "Processed 1051/4943 batches...\n",
      "Processed 1061/4943 batches...\n",
      "Processed 1071/4943 batches...\n",
      "Processed 1081/4943 batches...\n",
      "Processed 1091/4943 batches...\n",
      "Processed 1101/4943 batches...\n",
      "Processed 1111/4943 batches...\n",
      "Processed 1121/4943 batches...\n",
      "Processed 1131/4943 batches...\n",
      "Processed 1141/4943 batches...\n",
      "Processed 1151/4943 batches...\n",
      "Processed 1161/4943 batches...\n",
      "Processed 1171/4943 batches...\n",
      "Processed 1181/4943 batches...\n",
      "Processed 1191/4943 batches...\n",
      "Processed 1201/4943 batches...\n",
      "Processed 1211/4943 batches...\n",
      "Processed 1221/4943 batches...\n",
      "Processed 1231/4943 batches...\n",
      "Processed 1241/4943 batches...\n",
      "Processed 1251/4943 batches...\n",
      "Processed 1261/4943 batches...\n",
      "Processed 1271/4943 batches...\n",
      "Processed 1281/4943 batches...\n",
      "Processed 1291/4943 batches...\n",
      "Processed 1301/4943 batches...\n",
      "Processed 1311/4943 batches...\n",
      "Processed 1321/4943 batches...\n",
      "Processed 1331/4943 batches...\n",
      "Processed 1341/4943 batches...\n",
      "Processed 1351/4943 batches...\n",
      "Processed 1361/4943 batches...\n",
      "Processed 1371/4943 batches...\n",
      "Processed 1381/4943 batches...\n",
      "Processed 1391/4943 batches...\n",
      "Processed 1401/4943 batches...\n",
      "Processed 1411/4943 batches...\n",
      "Processed 1421/4943 batches...\n",
      "Processed 1431/4943 batches...\n",
      "Processed 1441/4943 batches...\n",
      "Processed 1451/4943 batches...\n",
      "Processed 1461/4943 batches...\n",
      "Processed 1471/4943 batches...\n",
      "Processed 1481/4943 batches...\n",
      "Processed 1491/4943 batches...\n",
      "Processed 1501/4943 batches...\n",
      "Processed 1511/4943 batches...\n",
      "Processed 1521/4943 batches...\n",
      "Processed 1531/4943 batches...\n",
      "Processed 1541/4943 batches...\n",
      "Processed 1551/4943 batches...\n",
      "Processed 1561/4943 batches...\n",
      "Processed 1571/4943 batches...\n",
      "Processed 1581/4943 batches...\n",
      "Processed 1591/4943 batches...\n",
      "Processed 1601/4943 batches...\n",
      "Processed 1611/4943 batches...\n",
      "Processed 1621/4943 batches...\n",
      "Processed 1631/4943 batches...\n",
      "Processed 1641/4943 batches...\n",
      "Processed 1651/4943 batches...\n",
      "Processed 1661/4943 batches...\n",
      "Processed 1671/4943 batches...\n",
      "Processed 1681/4943 batches...\n",
      "Processed 1691/4943 batches...\n",
      "Processed 1701/4943 batches...\n",
      "Processed 1711/4943 batches...\n",
      "Processed 1721/4943 batches...\n",
      "Processed 1731/4943 batches...\n",
      "Processed 1741/4943 batches...\n",
      "Processed 1751/4943 batches...\n",
      "Processed 1761/4943 batches...\n",
      "Processed 1771/4943 batches...\n",
      "Processed 1781/4943 batches...\n",
      "Processed 1791/4943 batches...\n",
      "Processed 1801/4943 batches...\n",
      "Processed 1811/4943 batches...\n",
      "Processed 1821/4943 batches...\n",
      "Processed 1831/4943 batches...\n",
      "Processed 1841/4943 batches...\n",
      "Processed 1851/4943 batches...\n",
      "Processed 1861/4943 batches...\n",
      "Processed 1871/4943 batches...\n",
      "Processed 1881/4943 batches...\n",
      "Processed 1891/4943 batches...\n",
      "Processed 1901/4943 batches...\n",
      "Processed 1911/4943 batches...\n",
      "Processed 1921/4943 batches...\n",
      "Processed 1931/4943 batches...\n",
      "Processed 1941/4943 batches...\n",
      "Processed 1951/4943 batches...\n",
      "Processed 1961/4943 batches...\n",
      "Processed 1971/4943 batches...\n",
      "Processed 1981/4943 batches...\n",
      "Processed 1991/4943 batches...\n",
      "Processed 2001/4943 batches...\n",
      "Processed 2011/4943 batches...\n",
      "Processed 2021/4943 batches...\n",
      "Processed 2031/4943 batches...\n",
      "Processed 2041/4943 batches...\n",
      "Processed 2051/4943 batches...\n",
      "Processed 2061/4943 batches...\n",
      "Processed 2071/4943 batches...\n",
      "Processed 2081/4943 batches...\n",
      "Processed 2091/4943 batches...\n",
      "Processed 2101/4943 batches...\n",
      "Processed 2111/4943 batches...\n",
      "Processed 2121/4943 batches...\n",
      "Processed 2131/4943 batches...\n",
      "Processed 2141/4943 batches...\n",
      "Processed 2151/4943 batches...\n",
      "Processed 2161/4943 batches...\n",
      "Processed 2171/4943 batches...\n",
      "Processed 2181/4943 batches...\n",
      "Processed 2191/4943 batches...\n",
      "Processed 2201/4943 batches...\n",
      "Processed 2211/4943 batches...\n",
      "Processed 2221/4943 batches...\n",
      "Processed 2231/4943 batches...\n",
      "Processed 2241/4943 batches...\n",
      "Processed 2251/4943 batches...\n",
      "Processed 2261/4943 batches...\n",
      "Processed 2271/4943 batches...\n",
      "Processed 2281/4943 batches...\n",
      "Processed 2291/4943 batches...\n",
      "Processed 2301/4943 batches...\n",
      "Processed 2311/4943 batches...\n",
      "Processed 2321/4943 batches...\n",
      "Processed 2331/4943 batches...\n",
      "Processed 2341/4943 batches...\n",
      "Processed 2351/4943 batches...\n",
      "Processed 2361/4943 batches...\n",
      "Processed 2371/4943 batches...\n",
      "Processed 2381/4943 batches...\n",
      "Processed 2391/4943 batches...\n",
      "Processed 2401/4943 batches...\n",
      "Processed 2411/4943 batches...\n",
      "Processed 2421/4943 batches...\n",
      "Processed 2431/4943 batches...\n",
      "Processed 2441/4943 batches...\n",
      "Processed 2451/4943 batches...\n",
      "Processed 2461/4943 batches...\n",
      "Processed 2471/4943 batches...\n",
      "Processed 2481/4943 batches...\n",
      "Processed 2491/4943 batches...\n",
      "Processed 2501/4943 batches...\n",
      "Processed 2511/4943 batches...\n",
      "Processed 2521/4943 batches...\n",
      "Processed 2531/4943 batches...\n",
      "Processed 2541/4943 batches...\n",
      "Processed 2551/4943 batches...\n",
      "Processed 2561/4943 batches...\n",
      "Processed 2571/4943 batches...\n",
      "Processed 2581/4943 batches...\n",
      "Processed 2591/4943 batches...\n",
      "Processed 2601/4943 batches...\n",
      "Processed 2611/4943 batches...\n",
      "Processed 2621/4943 batches...\n",
      "Processed 2631/4943 batches...\n",
      "Processed 2641/4943 batches...\n",
      "Processed 2651/4943 batches...\n",
      "Processed 2661/4943 batches...\n",
      "Processed 2671/4943 batches...\n",
      "Processed 2681/4943 batches...\n",
      "Processed 2691/4943 batches...\n",
      "Processed 2701/4943 batches...\n",
      "Processed 2711/4943 batches...\n",
      "Processed 2721/4943 batches...\n",
      "Processed 2731/4943 batches...\n"
     ]
    }
   ],
   "source": [
    "trainer.create_subsampled_datasets(\n",
    "    dataset='stead',\n",
    "    output_dir='preprocessed_data/stead_splits',\n",
    "    noise_percentages=[], \n",
    "    subsampling_factor=1, \n",
    "    maintain_constant_size=False,\n",
    "    save_train_val_test_splits=True, \n",
    "    val_ratio=0.2, \n",
    "    test_ratio=0.2,\n",
    "    random_state_mode='pseudorandom',\n",
    "    base_random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5c88046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "Train batches: 25\n",
      "Val batches: 10\n",
      "Using HDF5Generator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 18:09:40.183591: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 18:09:58.790858: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fd07c008940 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-07-04 18:09:58.790873: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3090 Ti, Compute Capability 8.6\n",
      "2025-07-04 18:09:58.793882: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-07-04 18:09:58.840333: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - ETA: 0s - loss: 2.4309\n",
      "Epoch 1: val_loss improved from inf to 2.05544, saving model to checkpoints/FULL_DATASET_SUBSAMPLED_1_train_best_model.h5\n",
      "25/25 [==============================] - 29s 290ms/step - loss: 2.4309 - val_loss: 2.0554\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - ETA: 0s - loss: 1.9864\n",
      "Epoch 2: val_loss did not improve from 2.05544\n",
      "25/25 [==============================] - 3s 136ms/step - loss: 1.9864 - val_loss: 2.4397\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - ETA: 0s - loss: 1.7570\n",
      "Epoch 3: val_loss improved from 2.05544 to 1.66183, saving model to checkpoints/FULL_DATASET_SUBSAMPLED_1_train_best_model.h5\n",
      "25/25 [==============================] - 4s 141ms/step - loss: 1.7570 - val_loss: 1.6618\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - ETA: 0s - loss: 1.6612\n",
      "Epoch 4: val_loss improved from 1.66183 to 1.64978, saving model to checkpoints/FULL_DATASET_SUBSAMPLED_1_train_best_model.h5\n",
      "25/25 [==============================] - 4s 142ms/step - loss: 1.6612 - val_loss: 1.6498\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - ETA: 0s - loss: 1.5980\n",
      "Epoch 5: val_loss improved from 1.64978 to 1.60637, saving model to checkpoints/FULL_DATASET_SUBSAMPLED_1_train_best_model.h5\n",
      "25/25 [==============================] - 4s 141ms/step - loss: 1.5980 - val_loss: 1.6064\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - ETA: 0s - loss: 1.5665\n",
      "Epoch 6: val_loss did not improve from 1.60637\n",
      "25/25 [==============================] - 3s 136ms/step - loss: 1.5665 - val_loss: 1.6072\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - ETA: 0s - loss: 1.5386\n",
      "Epoch 7: val_loss improved from 1.60637 to 1.54662, saving model to checkpoints/FULL_DATASET_SUBSAMPLED_1_train_best_model.h5\n",
      "25/25 [==============================] - 4s 141ms/step - loss: 1.5386 - val_loss: 1.5466\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - ETA: 0s - loss: 1.5189\n",
      "Epoch 8: val_loss improved from 1.54662 to 1.51729, saving model to checkpoints/FULL_DATASET_SUBSAMPLED_1_train_best_model.h5\n",
      "25/25 [==============================] - 4s 141ms/step - loss: 1.5189 - val_loss: 1.5173\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - ETA: 0s - loss: 1.4928\n",
      "Epoch 9: val_loss did not improve from 1.51729\n",
      "25/25 [==============================] - 3s 136ms/step - loss: 1.4928 - val_loss: 1.5190\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - ETA: 0s - loss: 1.4758\n",
      "Epoch 10: val_loss improved from 1.51729 to 1.48229, saving model to checkpoints/FULL_DATASET_SUBSAMPLED_1_train_best_model.h5\n",
      "25/25 [==============================] - 4s 142ms/step - loss: 1.4758 - val_loss: 1.4823\n"
     ]
    }
   ],
   "source": [
    "model = RepresentationLearningMultipleAutoencoder()\n",
    "\n",
    "history = trainer.train(\n",
    "    model=model,\n",
    "    train_dataset_path='preprocessed_data/stead_splits/FULL_DATASET_SUBSAMPLED_1_train.hdf5',\n",
    "    val_dataset_path='preprocessed_data/stead_splits/FULL_DATASET_SUBSAMPLED_1_val.hdf5',\n",
    "    test_dataset_path='preprocessed_data/stead_splits/FULL_DATASET_SUBSAMPLED_1_test.hdf5',  # Optional\n",
    "    epochs=10,\n",
    "    batch_size=256,\n",
    "    learning_rate=1e-3,\n",
    "    use_hdf5_generator=True  \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RECOVAR_EGE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
