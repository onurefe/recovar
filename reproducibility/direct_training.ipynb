{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1cfd6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 14:49:44.349627: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-04 14:49:44.367739: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-04 14:49:44.367759: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-04 14:49:44.367770: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-04 14:49:44.371220: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from direct_trainer import DirectTrainer\n",
    "from recovar import RepresentationLearningSingleAutoencoder, RepresentationLearningDenoisingSingleAutoencoder, RepresentationLearningMultipleAutoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feaf54af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from directory import (\n",
    "    STEAD_WAVEFORMS_HDF5_PATH,\n",
    "    STEAD_METADATA_CSV_PATH,\n",
    "    INSTANCE_EQ_WAVEFORMS_HDF5_PATH,\n",
    "    INSTANCE_NOISE_WAVEFORMS_HDF5_PATH,\n",
    "    INSTANCE_EQ_METADATA_CSV_PATH,\n",
    "    INSTANCE_NOISE_METADATA_CSV_PATH,\n",
    "    PREPROCESSED_DATASET_DIRECTORY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa67e348",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = DirectTrainer(dataset='stead',\n",
    "    dataset_time_window=60.0,  # STEAD has 60s windows\n",
    "    model_time_window=30.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c3cea72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ege/recovar/reproducibility/direct_trainer.py:266: DtypeWarning: Columns (21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  metadata = pd.read_csv(metadata_csv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples before subsampling: EQ=1030231, NO=235426\n",
      "\n",
      "Least common denominator (constant dataset size): 235426\n",
      "\n",
      "Creating dataset: SUBSAMPLED_100_NOISE_0.hdf5\n",
      "Total samples: 235426 (EQ: 235426, NO: 0)\n",
      "Processed 1/919 batches...\n",
      "Processed 11/919 batches...\n",
      "Processed 21/919 batches...\n",
      "Processed 31/919 batches...\n",
      "Processed 41/919 batches...\n",
      "Processed 51/919 batches...\n",
      "Processed 61/919 batches...\n",
      "Processed 71/919 batches...\n",
      "Processed 81/919 batches...\n",
      "Processed 91/919 batches...\n",
      "Processed 101/919 batches...\n",
      "Processed 111/919 batches...\n",
      "Processed 121/919 batches...\n",
      "Processed 131/919 batches...\n",
      "Processed 141/919 batches...\n",
      "Processed 151/919 batches...\n",
      "Processed 161/919 batches...\n",
      "Processed 171/919 batches...\n",
      "Processed 181/919 batches...\n",
      "Processed 191/919 batches...\n",
      "Processed 201/919 batches...\n",
      "Processed 211/919 batches...\n",
      "Processed 221/919 batches...\n",
      "Processed 231/919 batches...\n",
      "Processed 241/919 batches...\n",
      "Processed 251/919 batches...\n",
      "Processed 261/919 batches...\n",
      "Processed 271/919 batches...\n",
      "Processed 281/919 batches...\n",
      "Processed 291/919 batches...\n",
      "Processed 301/919 batches...\n",
      "Processed 311/919 batches...\n",
      "Processed 321/919 batches...\n",
      "Processed 331/919 batches...\n",
      "Processed 341/919 batches...\n",
      "Processed 351/919 batches...\n",
      "Processed 361/919 batches...\n",
      "Processed 371/919 batches...\n",
      "Processed 381/919 batches...\n",
      "Processed 391/919 batches...\n",
      "Processed 401/919 batches...\n",
      "Processed 411/919 batches...\n",
      "Processed 421/919 batches...\n",
      "Processed 431/919 batches...\n",
      "Processed 441/919 batches...\n",
      "Processed 451/919 batches...\n",
      "Processed 461/919 batches...\n",
      "Processed 471/919 batches...\n",
      "Processed 481/919 batches...\n",
      "Processed 491/919 batches...\n",
      "Processed 501/919 batches...\n",
      "Processed 511/919 batches...\n",
      "Processed 521/919 batches...\n",
      "Processed 531/919 batches...\n",
      "Processed 541/919 batches...\n",
      "Processed 551/919 batches...\n",
      "Processed 561/919 batches...\n",
      "Processed 571/919 batches...\n",
      "Processed 581/919 batches...\n",
      "Processed 591/919 batches...\n",
      "Processed 601/919 batches...\n",
      "Processed 611/919 batches...\n",
      "Processed 621/919 batches...\n",
      "Processed 631/919 batches...\n",
      "Processed 641/919 batches...\n",
      "Processed 651/919 batches...\n",
      "Processed 661/919 batches...\n",
      "Processed 671/919 batches...\n",
      "Processed 681/919 batches...\n",
      "Processed 691/919 batches...\n",
      "Processed 701/919 batches...\n",
      "Processed 711/919 batches...\n",
      "Processed 721/919 batches...\n",
      "Processed 731/919 batches...\n",
      "Processed 741/919 batches...\n",
      "Processed 751/919 batches...\n",
      "Processed 761/919 batches...\n",
      "Processed 771/919 batches...\n",
      "Processed 781/919 batches...\n",
      "Processed 791/919 batches...\n",
      "Processed 801/919 batches...\n",
      "Processed 811/919 batches...\n",
      "Processed 821/919 batches...\n",
      "Processed 831/919 batches...\n",
      "Processed 841/919 batches...\n",
      "Processed 851/919 batches...\n",
      "Processed 861/919 batches...\n",
      "Processed 871/919 batches...\n",
      "Processed 881/919 batches...\n",
      "Processed 891/919 batches...\n",
      "Processed 901/919 batches...\n",
      "Processed 911/919 batches...\n",
      "Dataset saved: /home/ege/recovar_data_preprocessed/SUBSAMPLED_100_NOISE_0.hdf5\n",
      "Created dataset with 0% noise (actual: 0.0%): EQ=235426, NO=0, Total=235426\n",
      "\n",
      "Creating dataset: SUBSAMPLED_100_NOISE_10.hdf5\n",
      "Total samples: 235426 (EQ: 211884, NO: 23542)\n",
      "Processed 1/919 batches...\n",
      "Processed 11/919 batches...\n",
      "Processed 21/919 batches...\n",
      "Processed 31/919 batches...\n",
      "Processed 41/919 batches...\n",
      "Processed 51/919 batches...\n",
      "Processed 61/919 batches...\n",
      "Processed 71/919 batches...\n",
      "Processed 81/919 batches...\n",
      "Processed 91/919 batches...\n",
      "Processed 101/919 batches...\n",
      "Processed 111/919 batches...\n",
      "Processed 121/919 batches...\n",
      "Processed 131/919 batches...\n",
      "Processed 141/919 batches...\n",
      "Processed 151/919 batches...\n",
      "Processed 161/919 batches...\n",
      "Processed 171/919 batches...\n",
      "Processed 181/919 batches...\n",
      "Processed 191/919 batches...\n",
      "Processed 201/919 batches...\n",
      "Processed 211/919 batches...\n",
      "Processed 221/919 batches...\n",
      "Processed 231/919 batches...\n",
      "Processed 241/919 batches...\n",
      "Processed 251/919 batches...\n",
      "Processed 261/919 batches...\n",
      "Processed 271/919 batches...\n",
      "Processed 281/919 batches...\n",
      "Processed 291/919 batches...\n",
      "Processed 301/919 batches...\n",
      "Processed 311/919 batches...\n",
      "Processed 321/919 batches...\n",
      "Processed 331/919 batches...\n",
      "Processed 341/919 batches...\n",
      "Processed 351/919 batches...\n",
      "Processed 361/919 batches...\n",
      "Processed 371/919 batches...\n",
      "Processed 381/919 batches...\n",
      "Processed 391/919 batches...\n",
      "Processed 401/919 batches...\n",
      "Processed 411/919 batches...\n",
      "Processed 421/919 batches...\n",
      "Processed 431/919 batches...\n",
      "Processed 441/919 batches...\n",
      "Processed 451/919 batches...\n",
      "Processed 461/919 batches...\n",
      "Processed 471/919 batches...\n",
      "Processed 481/919 batches...\n",
      "Processed 491/919 batches...\n",
      "Processed 501/919 batches...\n",
      "Processed 511/919 batches...\n",
      "Processed 521/919 batches...\n",
      "Processed 531/919 batches...\n",
      "Processed 541/919 batches...\n",
      "Processed 551/919 batches...\n",
      "Processed 561/919 batches...\n",
      "Processed 571/919 batches...\n",
      "Processed 581/919 batches...\n",
      "Processed 591/919 batches...\n",
      "Processed 601/919 batches...\n",
      "Processed 611/919 batches...\n",
      "Processed 621/919 batches...\n",
      "Processed 631/919 batches...\n",
      "Processed 641/919 batches...\n",
      "Processed 651/919 batches...\n",
      "Processed 661/919 batches...\n",
      "Processed 671/919 batches...\n",
      "Processed 681/919 batches...\n",
      "Processed 691/919 batches...\n",
      "Processed 701/919 batches...\n",
      "Processed 711/919 batches...\n",
      "Processed 721/919 batches...\n",
      "Processed 731/919 batches...\n",
      "Processed 741/919 batches...\n",
      "Processed 751/919 batches...\n",
      "Processed 761/919 batches...\n",
      "Processed 771/919 batches...\n",
      "Processed 781/919 batches...\n",
      "Processed 791/919 batches...\n",
      "Processed 801/919 batches...\n",
      "Processed 811/919 batches...\n",
      "Processed 821/919 batches...\n",
      "Processed 831/919 batches...\n",
      "Processed 841/919 batches...\n",
      "Processed 851/919 batches...\n",
      "Processed 861/919 batches...\n",
      "Processed 871/919 batches...\n",
      "Processed 881/919 batches...\n",
      "Processed 891/919 batches...\n",
      "Processed 901/919 batches...\n",
      "Processed 911/919 batches...\n",
      "Dataset saved: /home/ege/recovar_data_preprocessed/SUBSAMPLED_100_NOISE_10.hdf5\n",
      "Created dataset with 10% noise (actual: 10.0%): EQ=211884, NO=23542, Total=235426\n",
      "\n",
      "Creating dataset: SUBSAMPLED_100_NOISE_20.hdf5\n",
      "Total samples: 235426 (EQ: 188341, NO: 47085)\n",
      "Processed 1/919 batches...\n",
      "Processed 11/919 batches...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_subsampled_datasets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstead\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPREPROCESSED_DATASET_DIRECTORY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnoise_percentages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m90\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubsampling_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaintain_constant_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/recovar/reproducibility/direct_trainer.py:184\u001b[0m, in \u001b[0;36mDirectTrainer.create_subsampled_datasets\u001b[0;34m(self, dataset, output_dir, noise_percentages, subsampling_factor, maintain_constant_size)\u001b[0m\n\u001b[1;32m    181\u001b[0m combined_metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_crop_offsets(combined_metadata)\n\u001b[1;32m    183\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSUBSAMPLED_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(subsampling_factor\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_NOISE_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(noise_pct)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 184\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_preprocessed_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcombined_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43meq_hdf5_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_hdf5_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# Calculate actual percentage for verification\u001b[39;00m\n\u001b[1;32m    192\u001b[0m actual_noise_pct \u001b[38;5;241m=\u001b[39m (n_no \u001b[38;5;241m/\u001b[39m (n_no \u001b[38;5;241m+\u001b[39m n_eq) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m (n_no \u001b[38;5;241m+\u001b[39m n_eq) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/recovar/reproducibility/direct_trainer.py:472\u001b[0m, in \u001b[0;36mDirectTrainer._save_preprocessed_dataset\u001b[0;34m(self, metadata, eq_hdf5_path, no_hdf5_path, output_path)\u001b[0m\n\u001b[1;32m    470\u001b[0m sample_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_batches):\n\u001b[0;32m--> 472\u001b[0m     X_batch, y_batch \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_gen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# Store in HDF5\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m X_batch\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/recovar/reproducibility/data_generator.py:134\u001b[0m, in \u001b[0;36mBatchGenerator.get_batch\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    idx (int): Index of the batch.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m        and y_batch is the batch of labels.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    130\u001b[0m batch_waveforms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwaveforms[\n\u001b[1;32m    131\u001b[0m     (idx \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size) : ((idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[1;32m    132\u001b[0m ]\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_batchx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_waveforms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_batchy(batch_waveforms)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_batch\n",
      "File \u001b[0;32m~/recovar/reproducibility/data_generator.py:199\u001b[0m, in \u001b[0;36mBatchGenerator._get_batchx\u001b[0;34m(self, batch_waveforms)\u001b[0m\n\u001b[1;32m    196\u001b[0m     x\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_pick[waveform[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrace_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m waveform[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 199\u001b[0m     x\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_noise\u001b[49m\u001b[43m[\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrace_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    201\u001b[0m crop_offset \u001b[38;5;241m=\u001b[39m waveform[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrop_offset\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    202\u001b[0m crop_offsets\u001b[38;5;241m.\u001b[39mappend(crop_offset)\n",
      "File \u001b[0;32mh5py/_objects.pyx:56\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:57\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/RECOVAR_EGE/lib/python3.10/site-packages/h5py/_hl/group.py:360\u001b[0m, in \u001b[0;36mGroup.__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid HDF5 object reference\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m)):\n\u001b[0;32m--> 360\u001b[0m     oid \u001b[38;5;241m=\u001b[39m \u001b[43mh5o\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_e\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccessing a group is done with bytes or str, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(name)))\n",
      "File \u001b[0;32mh5py/_objects.pyx:56\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:57\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5o.pyx:257\u001b[0m, in \u001b[0;36mh5py.h5o.open\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5i.pyx:44\u001b[0m, in \u001b[0;36mh5py.h5i.wrap_identifier\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:404\u001b[0m, in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.create_subsampled_datasets(\n",
    "    dataset='stead',\n",
    "    output_dir=PREPROCESSED_DATASET_DIRECTORY,\n",
    "    noise_percentages=[0, 10, 20, 90, 100],\n",
    "    subsampling_factor=1.0, \n",
    "    maintain_constant_size=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6092e1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 14:49:50.329567: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 14:49:50.343959: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 14:49:50.344045: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 14:49:50.345038: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 14:49:50.345107: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 14:49:50.345146: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 14:49:50.618070: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 14:49:50.618146: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 14:49:50.618193: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 14:49:50.618234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22141 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 176448 samples (EQ: 158800, NO: 17648)\n",
      "Val: 58816 samples (EQ: 52933, NO: 5883)\n",
      "\n",
      "Starting training...\n",
      "Train samples: 176448\n",
      "Val samples: 58816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 14:49:55.519820: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n",
      "2025-07-04 14:49:59.483924: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7faf0c008d40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-07-04 14:49:59.483950: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3090 Ti, Compute Capability 8.6\n",
      "2025-07-04 14:49:59.487480: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-07-04 14:49:59.532927: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "690/690 [==============================] - ETA: 0s - loss: 1.0119\n",
      "Epoch 1: val_loss improved from inf to 0.92080, saving model to checkpoints/best_model.h5\n",
      "690/690 [==============================] - 26s 29ms/step - loss: 1.0119 - val_loss: 0.9208 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "trainer = DirectTrainer(dataset='stead', dataset_time_window=60.0, model_time_window=30.0)\n",
    "model = RepresentationLearningSingleAutoencoder()\n",
    "\n",
    "history = trainer.train(\n",
    "    model=model,\n",
    "    dataset_path=\"/home/ege/recovar_data_preprocessed/stead_pseudorandom_state_ne_subsampled/SUBSAMPLED_100_NOISE_10.hdf5\",\n",
    "    epochs=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RECOVAR_EGE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
